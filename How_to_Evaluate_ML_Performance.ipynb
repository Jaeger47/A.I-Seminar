{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How_to_Evaluate_ML_Performance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaeger47/A.I-Seminar/blob/main/How_to_Evaluate_ML_Performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL3Hs_LlJQAF"
      },
      "source": [
        "# I. Introduction\n",
        "\n",
        "After training your machine learning model, you need to know how well it performs on unseen data. There are 2 ways to evaluate the performance of an algorithm:\n",
        "* **Using Resampling Methods**. Use resampling methods that allow you to make accurate estimates for how well your algorithm will perform on new data. \n",
        "* **Using Algorithm Performance Metrics**. Make predictions for new data to which you already know the answers. This data is your test or validation dataset.\n",
        "\n",
        "<br>\n",
        "\n",
        "After completing this notebook, you will learn:\n",
        "* How to estimate the accuracy of machine learning algorithms using resampling methods in Python and scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoBaW3TfHqXl"
      },
      "source": [
        "## Download the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2NvZ29q7QSv"
      },
      "source": [
        "Before proceeding to the next sections, you need to download the dataset that will be used in the exercises. This is provided in our Google Classroom. You  have 2 options how to access the datasets in your notebook:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqS5rXcI_ryk"
      },
      "source": [
        "### Option 1: Upload the data from your Local File System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imcOeUz6_41Z",
        "outputId": "36b67e1e-03d8-4e56-966f-de8037964fd4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Uploading the data from Local File System\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0e5a5be3-9059-4276-893e-b0c12ae64a14\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0e5a5be3-9059-4276-893e-b0c12ae64a14\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving housing.csv to housing.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwGtsH9L-3NU"
      },
      "source": [
        "### Option 2: Mount your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FZ73dT2HpLg"
      },
      "source": [
        "# Mount your google drive and copy the authentication key to allow access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Locate the file in your Google Drive directory\n",
        "%cd drive/My\\ Drive/Colab\\ Notebooks/ML\\ training   # replace this line depending on the directory setup of your google drive\n",
        "\n",
        "# Uncomment this if you want to list files in the directory to check if the file is there\n",
        "# %ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAcJfYmwCY-Q"
      },
      "source": [
        "# II. Evaluate the Performance of Machine Learning Algorithms with Resampling\n",
        "\n",
        "There are 4 different techniques that we can use to split up our training dataset:\n",
        "\n",
        "\n",
        "*   Train and Test Sets.\n",
        "*   *k*-fold Cross Validation\n",
        "*   Leave One Out Cross Validation\n",
        "*   Repeated Random Test-Train Splits\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybHXwjIhDYhP"
      },
      "source": [
        "## **1. Split into Train and Test Sets**\n",
        "---\n",
        "* This is the simplest method to evaluate the performance of a machine learning algorithm.\n",
        "* This algorithm evaluation technique is very fast and ideal for large datasets (millions of records) where there is strong evidence that both splits of the data are representative of the underlying problem. \n",
        "* Because of the speed, it is useful to use this approach when the algorithm you are investigating is slow to train. \n",
        "* A downside of this technique is that it can have a high variance. This means that differences in the training and test dataset can result in meaningful differences in the estimate of accuracy.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMJYbH96DfOs"
      },
      "source": [
        "**The Method:**\n",
        "* Split the original dataset into 2 parts: train and test\n",
        "* Train on the first part then make predictions on the second part\n",
        "* It is common to use 67% of the data for training and the remaining 33% for testing \n",
        "<br><br>\n",
        "<t>\n",
        "<img src='https://drive.google.com/uc?export=view&id=179hlspS49G7drY99VY6ydBDj6HMA6XwU' width=500px>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2gOGRIHDG03"
      },
      "source": [
        "In the example below we split the Pima Indians dataset into 67%/33% splits for training and test and evaluate the accuracy of a Logistic Regression model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPpq_tiZc-n2",
        "outputId": "e3f1d15d-5018-4323-94dd-d57159b6821e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Evaluate using a train and a test set\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset \n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names, comment='#')\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# Set the test size\n",
        "test_size = 0.33\n",
        "seed = 7\n",
        "\n",
        "# Split the dataset into test and train \n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Train the data on a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=130) \n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "result = model.score(X_test, Y_test)\n",
        "print((\"Accuracy: %.3f%%\") % (result*100.0))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 78.740%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5c-8EJwEeuD"
      },
      "source": [
        "*More info on scikit-learn's train_test_split function [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhJEfdI0R5ng"
      },
      "source": [
        "## **2. K-fold Cross Validation**\n",
        "---\n",
        "**Cross validation** is an approach that you can use to estimate the performance of machine learning algorithm with less variance than a single train-test set split. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-2t8Ky2pjL5"
      },
      "source": [
        "\n",
        "**The Method:**\n",
        "* Split the dataset into k-parts (e.\n",
        "g. k = 5 or k = 10). Each split of the data is called a **fold**. \n",
        "* The model is trained on *k − 1* folds with one held back and tested on the held back fold.  This is repeated so that each fold of the dataset is given a chance to be the held back test set. \n",
        "* After running cross validation you end up with *k* different performance scores that you can summarize using a mean and a standard deviation.\n",
        "\n",
        " <img src='https://drive.google.com/uc?export=view&id=1BLxuWeeaRyM6ztSOpXX_cqxpwDytdF0c' width=600px>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGzxPzP2lMOC"
      },
      "source": [
        "\n",
        "In the example below we use 10-fold cross validation on the Pima Indians Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgcm7sF9HS8C",
        "outputId": "55d5736b-fed5-4ff6-a0cd-0a2726bc9366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Evaluate using Cross Validation\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset \n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names, comment='#')\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# Set k or the number of folds\n",
        "num_folds = 10\n",
        "seed = 7\n",
        "\n",
        "# Split the dataset into k folds\n",
        "kfold = KFold(n_splits=num_folds, shuffle=False, random_state=None)\n",
        "\n",
        "# Train the data on a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=210)\n",
        "\n",
        "# Evaluate the score of a kfold cross validation splitting strategy\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print((\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 77.604% (5.158%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkKgHDAPV2Qz"
      },
      "source": [
        "## **3. Leave One Out Cross Validation**\n",
        "---\n",
        "* You can configure cross validation so that the \n",
        "size of the fold is 1 (_k is set to the number of\n",
        "observations in your dataset_). This variation of cross validation is called **leave-one-out cross validation**. \n",
        "\n",
        "* The result is a large number of performance measures that can be summarized in an effort to give a more reasonable estimate of the accuracy of your model on unseen data. \n",
        "\n",
        "* However, it can be a computationally more expensive procedure than k-fold cross validation. \n",
        "\n",
        " <img src='https://drive.google.com/uc?export=view&id=1Aat8S3aT1qw94jDUkh8z4Y8IEmJ4Cpqg' width=580px>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYWXrersqTZL"
      },
      "source": [
        "In the example below we use leave-one-out cross validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5ApIqHwSujR",
        "outputId": "2dd968d4-bae9-47e0-fcf8-1b7dda2a1cc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Evaluate using Leave One Out Cross Validation\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names, comment='#')\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# Split dataset into a Leave One Out Cross Validation\n",
        "loocv = LeaveOneOut()\n",
        "\n",
        "# Train the data on a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "\n",
        "# Evaluate the score of a leave one out cross validation split strategy \n",
        "results = cross_val_score(model, X, Y, cv=loocv)          # there are N scores, where N is the total no. of rows/fold in the dataset\n",
        "print((\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 77.604% (41.689%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxXaE4MTuha3",
        "outputId": "9afe6b01-e1ab-4a53-edde-271140bbdebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# How many folds are created?\n",
        "print(len(results))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05QI-iXNXh1U"
      },
      "source": [
        "## **4. Repeated Random Test-Train Splits**\n",
        "---\n",
        "* This is another variation on k-fold cross validation that creates a random split of the data like the train/test split described previously, but the process of splitting and evaluation of the algorithm is repeated multiple times on the whole dataset.\n",
        "\n",
        "* This has the speed of using a train/test split and the reduction in variance in the estimated performance of k-fold cross validation. \n",
        "\n",
        "* A down side is that repetitions may include much of the same data in the train or the test split from run to run,introducing redundancy into the evaluation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_0ON0jd7kFs"
      },
      "source": [
        "The example below splits the data into a 67%/33% train/test split and repeats the process 10 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihh9GsEDW4kz",
        "outputId": "9627bff8-6604-4f13-d964-1d510fd6a4bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Evaluate using Shuffle Split Cross Validation\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names, comment='#')\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# Set the number of splitting iterations and the test size\n",
        "n_splits = 10\n",
        "test_size = 0.33\n",
        "seed = 7\n",
        "\n",
        "# Shuffle and split dataset 'n_splits' times\n",
        "kfold = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Train the data on a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=180)\n",
        "\n",
        "# Evaluate the score of a repeated random test-train split strategy\n",
        "results = cross_val_score(model, X, Y, cv=kfold)\n",
        "print((\"Accuracy: %.3f%% (%.3f%%)\") % (results.mean()*100.0, results.std()*100.0))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 76.535% (2.235%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGtuIzg-q4nn"
      },
      "source": [
        "# III. Machine Learning Algorithm Performance Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tcvDh5GPrap"
      },
      "source": [
        "In this lesson, various different algorithm evaluation metrics are demonstrated for both classification and regression type machine learning problems. \n",
        "* For **classification metrics**, the _Pima Indians onset of diabetes dataset_ is used as demonstration. This is a binary classification problem where all of the input variables are\n",
        "numeric.\n",
        "* For **regression metrics**, the _Boston House Price dataset_ is used as demonstration. this is a regression problem where all of the input variables are also numeric.\n",
        "\n",
        "All recipes evaluate the same algorithms, Logistic Regression for classification and Linear Regression for the regression problems. A 10-fold cross validation test harness is used to demonstrate each metric, because this is the most likely scenario you will use when employing different algorithm evaluation metrics.\n",
        "\n",
        "You can learn more about machine learning algorithm performance metrics supported by\n",
        "scikit-learn on the page _Model evaluation: quantifying the quality of predictions_. \n",
        "\n",
        "Let’s get on with the evaluation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89gaKi86AvXK"
      },
      "source": [
        "## A. Classification Metrics\n",
        "\n",
        "Classification problems are perhaps the most common type of machine learning problem and as\n",
        "such there are a myriad of metrics that can be used to evaluate predictions for these problems.\n",
        "In this section we will review how to use the following metrics:\n",
        "* Classification Accuracy.\n",
        "* Logarithmic Loss.\n",
        "* Area Under ROC Curve.\n",
        "* Confusion Matrix.\n",
        "* Classification Report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6h-ppKRA7A4"
      },
      "source": [
        "### **1. Classification Accuracy**\n",
        "---\n",
        "* **Classification accuracy** is the number of correct predictions made as a ratio of all predictions\n",
        "made. This is the most common evaluation metric for classification problems, it is also the most\n",
        "misused. \n",
        "* It is really only suitable when there are an equal number of observations in each class\n",
        "(which is rarely the case) and that all predictions and prediction errors are equally important, which is often not the case. Below is an example of calculating classification accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBl2RyitYBPB",
        "outputId": "18da7dd6-c4e8-4cb3-8c89-5e231287fd7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cross Validation Classification Accuracy\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names, comment='#')\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# Split the dataset into a 10-fold cross validation\n",
        "kfold = KFold(n_splits=10, random_state=None)\n",
        "\n",
        "# Train the data on a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=210)\n",
        "\n",
        "# Calculate the classification accuracy\n",
        "scoring = 'accuracy'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print((\"Accuracy: %.3f (%.3f)\") % (results.mean(), results.std()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.776 (0.052)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpWaKGkJBtvh"
      },
      "source": [
        "### **2. Logarithmic Loss**\n",
        "---\n",
        "* **Logarithmic loss** (or log-loss) is a performance metric for evaluating the predictions of probabilities of membership to a given class.\n",
        "* It is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification)\n",
        "* The more the predicted probability diverges from the actual value, the higher is the log-loss value.\n",
        "* A lower log-loss value means better predictions.\n",
        "\n",
        " <img src='https://drive.google.com/uc?export=view&id=1UPXZhtSZJpfu8C-2w6rYuOt4h5EFoCcU' width=800px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b58kcWr4R_Ns"
      },
      "source": [
        "Below is an example of calculating log-loss for Logistic regression predictions on the Pima Indians onset of diabetes dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDTni1rzBM8X",
        "outputId": "0256ff99-fa65-4359-d391-05e6e1e0de3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cross Validation Classification LogLoss\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names, comment='#')\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# Split the dataset into a 10-fold cross validation\n",
        "kfold = KFold(n_splits=10, random_state=None)\n",
        "\n",
        "# Train the data on a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Calculate the log-loss\n",
        "scoring = 'neg_log_loss'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print((\"Logloss: %.3f (%.3f)\") % (results.mean(), results.std()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logloss: -0.484 (0.062)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhRJRpbwClXZ"
      },
      "source": [
        "*Smaller logloss is better with 0 representing a perfect logloss. The measure is inverted to be ascending when using the cross val score() function.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32EZL7KqCqCJ"
      },
      "source": [
        "### **3. Area Under ROC Curve**\n",
        "---\n",
        "*ROC: Receiver Operating Characteristic curve*\n",
        "\n",
        "\n",
        "* **Area under ROC Curve** (or AUC for short) is a performance metric for binary classification problems. \n",
        "* The AUC represents a model’s ability to discriminate between positive and negative classes. \n",
        "* An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model that is as good as random. \n",
        "* ROC can be broken down into sensitivity and specificity: \n",
        " * **Sensitivity** is the true positive rate also called the recall. It is the number of instances\n",
        "from the positive (first) class that actually predicted correctly.\n",
        " * **Specificity** is also called the true negative rate. Is the number of instances from the\n",
        "negative (second) class that were actually predicted correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mGQiTCIXGiO"
      },
      "source": [
        "\n",
        "The example below provides a demonstration of calculating AUC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq8d6czECM4y",
        "outputId": "f5d405a4-0c59-40b4-f549-58f960210934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cross Validation Classification ROC AUC\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names, comment='#')\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# Split the dataset into a 10-fold cross validation\n",
        "kfold = KFold(n_splits=10, random_state=None)\n",
        "\n",
        "# Train the data on a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=210) \n",
        "\n",
        "# Calculate the area under ROC\n",
        "scoring = 'roc_auc'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print((\"AUC: %.3f (%.3f)\") % (results.mean(), results.std()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.828 (0.043)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CSTCNbYKrqx"
      },
      "source": [
        "### **4. Confusion Matrix**\n",
        "---\n",
        "* The **confusion matrix** is a handy presentation of the accuracy of a model with two or more classes. \n",
        "* The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm.\n",
        "\n",
        " <img src='https://drive.google.com/uc?export=view&id=1fog1cY4PUgs70l10uhK_0UWvwabfmzkj' width=480px>\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chNJ3f4gYuHe"
      },
      "source": [
        "Below is an example of calculating a confusion matrix for a set of predictions by a Logistic Regression on the Pima Indians onset of diabetes dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra4l3M76DgwS",
        "outputId": "21679540-9da1-443d-ccbe-1621f74cf90c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cross Validation Classification Confusion Matrix\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names, comment='#')\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# Split the dataset into train and test\n",
        "test_size = 0.33\n",
        "seed = 7\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Train the data on a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=180)\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Calculate confusion matrix for a set of predictions\n",
        "predicted = model.predict(X_test)\n",
        "matrix = confusion_matrix(Y_test, predicted)\n",
        "print(matrix)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[142  20]\n",
            " [ 34  58]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0yJuWocLvBH"
      },
      "source": [
        "_Although the array is printed without headings, you can see that the majority of the predictions fall on the diagonal line of the matrix (which are correct predictions)._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqTWgiPZLu5X"
      },
      "source": [
        "### **5. Classification Report**\n",
        "---\n",
        "* The scikit-learn library provides a convenience report when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures. \n",
        "* The ```classification report()``` function displays the precision, recall, F1-score and support for each class. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqMuUAFlicMs"
      },
      "source": [
        "The example below demonstrates the report on the binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-WPFR25LSOc",
        "outputId": "96e1cadd-4c51-4721-abc0-9173375bf517",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cross Validation Classification Report\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'pima-indians-diabetes.csv'\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = read_csv(filename, names=names, comment='#')\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# Split the dataset into train and test\n",
        "test_size = 0.33\n",
        "seed = 7\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
        "\n",
        "# Train the data on a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=180)\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Get classification report \n",
        "predicted = model.predict(X_test)\n",
        "report = classification_report(Y_test, predicted)\n",
        "print(report)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.88      0.84       162\n",
            "         1.0       0.74      0.63      0.68        92\n",
            "\n",
            "    accuracy                           0.79       254\n",
            "   macro avg       0.78      0.75      0.76       254\n",
            "weighted avg       0.78      0.79      0.78       254\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5PFyVQWMo1v"
      },
      "source": [
        "## B. Regression Metrics\n",
        "In this section will review 3 of the most common metrics for evaluating predictions on regression\n",
        "machine learning problems:\n",
        "* Mean Absolute Error.\n",
        "* Mean Squared Error.\n",
        "* R2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9aQzrsuM3is"
      },
      "source": [
        "### **1. Mean Absolute Error**\n",
        "---\n",
        "* The **Mean Absolute Error** (or MAE) is the sum of the absolute differences between predictions and actual values. \n",
        "* It gives an idea of how wrong the predictions were. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ordf82NH-pi"
      },
      "source": [
        "The example below demonstrates calculating mean absolute error on the Boston house price [dataset](https://www.kaggle.com/vikrishnan/boston-house-prices)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0J3T3l6MU-5",
        "outputId": "955fd718-29de-4d2e-92f1-cc0016889df8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cross Validation Regression MAE\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "\n",
        "# Split the dataset into a 10-fold cross validation\n",
        "kfold = KFold(n_splits=10, random_state=None)\n",
        "\n",
        "# Train the data on a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Calculate the mean absolute error\n",
        "scoring = 'neg_mean_absolute_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print((\"MAE: %.3f (%.3f)\") % (results.mean(), results.std()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE: -4.005 (2.084)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-JIN5kCOeDj"
      },
      "source": [
        "A value of 0 indicates no error or perfect predictions. Like logloss, this metric is inverted by the cross val score() function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iami33ZKOjct"
      },
      "source": [
        "### **2. Mean Squared Error**\n",
        "---\n",
        "* The **Mean Squared Error** (or MSE) is much like the mean absolute error in that it provides a gross idea of the magnitude of error. \n",
        "* Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the **Root Mean Squared Error** (or RMSE). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyVemfQwI38p"
      },
      "source": [
        "The example below provides a demonstration of calculating mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmbTb4bBOMD7",
        "outputId": "4fd23419-92ba-4021-8b60-ba5a67832455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cross Validation Regression MSE\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "\n",
        "# Split the dataset into a 10-fold cross validation\n",
        "num_folds = 10\n",
        "kfold = KFold(n_splits=10, random_state=None)\n",
        "\n",
        "# Train the data on a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Caculate the mean squared error\n",
        "scoring = 'neg_mean_squared_error'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print((\"MSE: %.3f (%.3f)\") % (results.mean(), results.std()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE: -34.705 (45.574)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKBwzPK2X8wB"
      },
      "source": [
        "### **3. R2 Metric** \n",
        "\n",
        "* The **R2 (or R Squared) metric** provides an indication of the goodness of fit of a set of predictions to the actual values. \n",
        "* In statistical literature this measure is called the _coefficient of determination_. This is a value between 0 and 1 for no-fit and perfect fit respectively. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j2jGJ1FJPUO"
      },
      "source": [
        "The example below provides a demonstration of calculating the mean R2 for a set of predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80SRs0pRO8Ri",
        "outputId": "cc2cb1c4-dd48-4a83-bd1d-98586e996fdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Cross Validation Regression R^2\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'housing.csv'\n",
        "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "dataframe = read_csv(filename, delim_whitespace=True, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:13]\n",
        "Y = array[:,13]\n",
        "\n",
        "# Split the dataset into a 10-fold cross validation\n",
        "kfold = KFold(n_splits=10, random_state=None)\n",
        "\n",
        "# Train the data on a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Calculate the R2 metric\n",
        "scoring = 'r2'\n",
        "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "print((\"R^2: %.3f (%.3f)\") % (results.mean(), results.std()))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R^2: 0.203 (0.595)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EadIZrwfPpPc"
      },
      "source": [
        "# IV. Summary\n",
        "\n",
        "In this notebook you discovered resampling techniques and performance metrics that you can use to evaluate your machine learning algorithms.\n",
        "\n",
        "You learned four resampling techniques:\n",
        "* Split into Train and Test Sets\n",
        "* K-fold Cross Validation\n",
        "* Leave One Out Cross Validation\n",
        "* Repeated Random Test-Train Splits\n",
        "\n",
        "You learned about three classification metrics: \n",
        "* Accuracy\n",
        "* Logarithmic Loss\n",
        "* Area Under ROC Curve \n",
        "\n",
        "You also learned about two convenience methods for classification prediction results: \n",
        "* Confusion Matrix \n",
        "* Classification Report \n",
        "\n",
        "Finally, you also learned about\n",
        "three metrics for regression problems: \n",
        "* Mean Absolute Error\n",
        "* Mean Squared Error \n",
        "* R2"
      ]
    }
  ]
}